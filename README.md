# Human-Robot Interaction System Using Sign Language Recognition
<img width="686" alt="image" src="https://github.com/user-attachments/assets/6b7b41b4-c125-4e08-9e12-794dace6e168">.


## Introduction
This project aims to develop a human-robot interaction system using sign language recognition, machine learning, and artificial intelligence-generated responses. The system identifies sign language gestures using a camera, translates these gestures into letters, builds words and sentences, and sends them to an AI model to generate a personalized response. The response is then displayed by a robotic hand, which shows each letter individually.

## Detailed Description of the System
The system is divided into several main components:
1. **Sign Language Recognition:** The system uses a camera to recognize and translate hand gestures into letters.
2. **Word and Sentence Construction:** The sequence of recognized letters is used to build words and sentences.
3. **Sending Queries to the AI Model:** After constructing the sentence, the system sends a query to the GEMINI model to generate a personalized response.
4. **Displaying the Response with the Robotic Hand:** The response received from the model is displayed by a robotic hand, showing each letter one by one.

## Conclusions
The project has achieved several key goals:
- Recognition of hand gestures using a camera.
- Translation of hand gestures into letters, building words and sentences, and sending them to the AI model.
- Generation of a personalized response using the GEMINI model.
- Integration with Arduino to send the recognized responses to the robotic hand, which visually displays the responses. The system operates efficiently and provides results close to the initial goals defined at the beginning of the project.

## Future Work
Future development of the project includes:
- Improving the accuracy of sign recognition through further training of the model with more diverse data.
- Adding support for additional sign languages beyond English.
- Creating a graphical user interface to enhance user experience.
- Linking the system with various applications for more effective communication.
- Enhancing the functionality of the robotic hand for more precise and faster response display.

 ## Images of the Letters Displayed by the Robot
 Here are images showing how the robotic hand displays each letter:
 # Letter A
![image](https://github.com/user-attachments/assets/97161ae3-3f26-41c2-93eb-ab78c5ac4f87).

 # Letter B
 ![image](https://github.com/user-attachments/assets/cb729fb2-4c5d-4584-ac01-5642ce1bc9ec).


 # Letter C
 ![image](https://github.com/user-attachments/assets/bc4b6cbe-b2a1-4dad-ba34-18a70f905bfc).


## Demo Video
For a demonstration of the system in action, please watch the following video:
[Watch the demo] (https://www.youtube.com/watch?v=DdISPnmMqYc)
